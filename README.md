
# ETL AWS Women STEM ğŸš€

Este repositorio forma parte de la presentaciÃ³n de la charla tÃ©cnica *"Transformando datos y realidades: tu primera experiencia con AWS Glue"*, impartida el 8 de mayo en **Amazon HQ, CDMX**.
Con el objetivo de inspirar a mujeres y personas que estÃ¡n comenzando en el mundo del anÃ¡lisis de datos, se expone el proceso de construcciÃ³n de un pipeline ETL utilizando AWS Glue, cuyos resultados se visualizan posteriormente en Amazon QuickSight.
El desarrollo se centra en el procesamiento de una base de datos con indicadores clave para comprender la brecha de gÃ©nero y la situaciÃ³n de las mujeres en el Ã¡mbito STEM, desde una perspectiva social y humanÃ­stica.

## ğŸ“– DescripciÃ³n

Este proyecto de ingenierÃ­a de datos permite automatizar procesos de extracciÃ³n, transformaciÃ³n e imputaciÃ³n de datos utilizando AWS Glue y Amazon QuickSight, enfocado especÃ­ficamente en datos sobre mujeres estudiantes y profesionistas en Ã¡reas STEM en AmÃ©rica Latina.

## ğŸ› ï¸ TecnologÃ­a Utilizada

* **AWS Glue**: para la creaciÃ³n y ejecuciÃ³n del ETL.
* **Amazon QuickSight**: para visualizaciÃ³n interactiva de resultados.
* **AWS S3**: almacenamiento de datos en la nube.
* **Python**: lenguaje de programaciÃ³n para scripts ETL.
* **Pandas y scikit-learn**: bibliotecas utilizadas para manejo e imputaciÃ³n de datos.

## ğŸ—‚ï¸ Estructura del Proyecto

```
. â”œâ”€â”€ reports/              # PresentaciÃ³n de desarrollo, resultados y grÃ¡ficos exportados
  â”œâ”€â”€ glue_job_script.py    # Script principal de Glue para ETL e imputaciÃ³n de datos.
  â”œâ”€â”€ manifest.json         # Archivo para especificar ubicaciÃ³n y formato de archivos en S3.
  â””â”€â”€ requirements.txt      # Dependencias necesarias para ejecuciÃ³n de librerias (opcional).
```

## ğŸ§± Arquitectura
![Arquitectura](arquitectura.png)



## ğŸš© Requisitos Previos

* Cuenta activa de AWS.
* Acceso configurado a AWS Glue y Amazon QuickSight.
* Bucket S3 previamente creado.

## ğŸ”§ CÃ³mo usar este repositorio

### 1ï¸âƒ£ Crea un bucket de origen y destino y carga de archivos en Amazon S3


### 2ï¸âƒ£ ConfiguraciÃ³n del Script en AWS Glue

* Accede a AWS Glue 
* Crea un crawler y guerda tus resultados en el data calog
* Crea un nuevo Job en Glue:

  * Tipo: Spark con Python 3
  * Rol IAM: Elige o crea uno con permisos adecuados (S3, Glue)
* Copia el cÃ³digo del archivo `glue_job_script.py` en el editor del Job.
* Ajusta rutas de S3 si es necesario.

### 3ï¸âƒ£ Ejecutar el Job

* Guarda y ejecuta tu Job.
* Verifica en CloudWatch o la pestaÃ±a "Runs" el estado del proceso.

### 4ï¸âƒ£ VisualizaciÃ³n en QuickSight

* Usa el archivo `manifest.json` para configurar conjuntos de datos en Amazon QuickSight.
* Conecta QuickSight al bucket S3 correspondiente.
* Crea visualizaciones para analizar resultados.

## ğŸ“ Ejemplo del Archivo Manifest

```json
{
  "fileLocations": [
    {"URIPrefixes": ["s3://target-woman-stem-latam/estudiantes_mice/"]},
    {"URIPrefixes": ["s3://target-woman-stem-latam/personal_mice/"]}
  ],
  "globalUploadSettings": {
    "format": "CSV"
  }
}
```

## ğŸ“Œ Dependencias Python (opcional)

Si necesitas ejecutar o probar localmente:

```bash
pip install -r requirements.txt
```

## ğŸ“ Soporte y contribuciones

Si tienes preguntas o sugerencias, Â¡no dudes en abrir un issue o enviar un pull request!

---

Â¡Gracias por utilizar este repositorio! ğŸŒŸ
